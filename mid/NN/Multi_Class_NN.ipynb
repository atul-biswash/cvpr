{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Multi-Class Neural Network Implementation\n",
				"\n",
				"This notebook implements a three hidden layer neural network for multi-class classification (5 classes) using **Sigmoid activation** for all layers."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. Imports and Data Generation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import numpy as np\n",
				"import matplotlib.pyplot as plt\n",
				"from sklearn.datasets import make_blobs\n",
				"from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
				"from sklearn.model_selection import train_test_split\n",
				"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
				"import seaborn as sns\n",
				"import pandas as pd\n",
				"\n",
				"def generate_data(n_samples=2000, n_classes=5, n_features=2, random_state=42):\n",
				"    \"\"\"\n",
				"    Generates a synthetic dataset.\n",
				"    \"\"\"\n",
				"    X, y_raw = make_blobs(n_samples=n_samples, centers=n_classes, n_features=n_features, random_state=random_state, cluster_std=1.5)\n",
				"    \n",
				"    # One-hot encode\n",
				"    encoder = OneHotEncoder(sparse_output=False)\n",
				"    y = encoder.fit_transform(y_raw.reshape(-1, 1))\n",
				"    \n",
				"    return X, y, y_raw\n",
				"\n",
				"# Generate Data\n",
				"print(\"Generating data...\")\n",
				"X, y, y_raw = generate_data()\n",
				"\n",
				"# Save to CSV\n",
				"df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(X.shape[1])])\n",
				"df['label'] = y_raw\n",
				"df.to_csv('synthetic_data.csv', index=False)\n",
				"print(\"Data saved to synthetic_data.csv\")\n",
				"\n",
				"# Load from CSV\n",
				"print(\"Loading data from synthetic_data.csv...\")\n",
				"df_loaded = pd.read_csv('synthetic_data.csv')\n",
				"X = df_loaded.drop('label', axis=1).values\n",
				"y_raw = df_loaded['label'].values\n",
				"\n",
				"# Re-encode labels (since we loaded raw labels)\n",
				"encoder = OneHotEncoder(sparse_output=False)\n",
				"y = encoder.fit_transform(y_raw.reshape(-1, 1))\n",
				"print(\"Data loaded and re-encoded.\")\n",
				"\n",
				"# Standardize Data\n",
				"scaler = StandardScaler()\n",
				"X = scaler.fit_transform(X)\n",
				"\n",
				"# Split Data\n",
				"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. Neural Network Class\n",
				"Implementing the Neural Network with 3 hidden layers and Sigmoid activation."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class NeuralNetwork(object):\n",
				"    def __init__(self, input_size=2, hidden_size=10, output_size=5, learning_rate=0.1):\n",
				"        \"\"\"\n",
				"        Initializes the Neural Network with 3 hidden layers.\n",
				"        \"\"\"\n",
				"        self.inputLayerNeurons = input_size\n",
				"        self.hiddenLayerNeurons = hidden_size\n",
				"        self.outLayerNeurons = output_size\n",
				"        self.learning_rate = learning_rate\n",
				"\n",
				"        # Initialize weights\n",
				"        # Layer 1: Input -> Hidden 1\n",
				"        self.W1 = np.random.randn(self.inputLayerNeurons, self.hiddenLayerNeurons)\n",
				"        # Layer 2: Hidden 1 -> Hidden 2\n",
				"        self.W2 = np.random.randn(self.hiddenLayerNeurons, self.hiddenLayerNeurons)\n",
				"        # Layer 3: Hidden 2 -> Hidden 3\n",
				"        self.W3 = np.random.randn(self.hiddenLayerNeurons, self.hiddenLayerNeurons)\n",
				"        # Layer 4: Hidden 3 -> Output\n",
				"        self.W4 = np.random.randn(self.hiddenLayerNeurons, self.outLayerNeurons)\n",
				"\n",
				"    def sigmoid(self, x, der=False):\n",
				"        if der == True:\n",
				"            return x * (1 - x)\n",
				"        else:\n",
				"            return 1 / (1 + np.exp(-x))\n",
				"\n",
				"    def feedForward(self, X):\n",
				"        # Hidden Layer 1\n",
				"        self.z1 = np.dot(X, self.W1)\n",
				"        self.a1 = self.sigmoid(self.z1)\n",
				"\n",
				"        # Hidden Layer 2\n",
				"        self.z2 = np.dot(self.a1, self.W2)\n",
				"        self.a2 = self.sigmoid(self.z2)\n",
				"\n",
				"        # Hidden Layer 3\n",
				"        self.z3 = np.dot(self.a2, self.W3)\n",
				"        self.a3 = self.sigmoid(self.z3)\n",
				"\n",
				"        # Output Layer\n",
				"        self.z4 = np.dot(self.a3, self.W4)\n",
				"        self.pred = self.sigmoid(self.z4)\n",
				"\n",
				"        return self.pred\n",
				"\n",
				"    def backPropagation(self, X, Y, pred):\n",
				"        # Output Layer Error\n",
				"        output_error = Y - pred\n",
				"        delta4 = self.learning_rate * output_error * self.sigmoid(pred, der=True)\n",
				"\n",
				"        # Hidden Layer 3 Error\n",
				"        error3 = np.dot(delta4, self.W4.T)\n",
				"        delta3 = self.learning_rate * error3 * self.sigmoid(self.a3, der=True)\n",
				"\n",
				"        # Hidden Layer 2 Error\n",
				"        error2 = np.dot(delta3, self.W3.T)\n",
				"        delta2 = self.learning_rate * error2 * self.sigmoid(self.a2, der=True)\n",
				"\n",
				"        # Hidden Layer 1 Error\n",
				"        error1 = np.dot(delta2, self.W2.T)\n",
				"        delta1 = self.learning_rate * error1 * self.sigmoid(self.a1, der=True)\n",
				"\n",
				"        # Update Weights\n",
				"        self.W4 += np.dot(self.a3.T, delta4)\n",
				"        self.W3 += np.dot(self.a2.T, delta3)\n",
				"        self.W2 += np.dot(self.a1.T, delta2)\n",
				"        self.W1 += np.dot(X.T, delta1)\n",
				"\n",
				"    def train(self, X, Y):\n",
				"        output = self.feedForward(X)\n",
				"        self.backPropagation(X, Y, output)\n",
				"        return output\n",
				"\n",
				"    def predict(self, X):\n",
				"        probs = self.feedForward(X)\n",
				"        return np.argmax(probs, axis=1)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. Helper Functions"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def plot_loss(loss_history):\n",
				"    plt.figure(figsize=(10, 6))\n",
				"    plt.plot(loss_history)\n",
				"    plt.title('Training Loss Over Epochs')\n",
				"    plt.xlabel('Epochs')\n",
				"    plt.ylabel('Loss')\n",
				"    plt.grid(True)\n",
				"    plt.show()\n",
				"\n",
				"def plot_confusion_matrix(y_true, y_pred, classes):\n",
				"    cm = confusion_matrix(y_true, y_pred)\n",
				"    plt.figure(figsize=(8, 6))\n",
				"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
				"    plt.title('Confusion Matrix')\n",
				"    plt.xlabel('Predicted')\n",
				"    plt.ylabel('Actual')\n",
				"    plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. Training and Evaluation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize Neural Network\n",
				"input_size = X.shape[1]\n",
				"output_size = y.shape[1]\n",
				"nn = NeuralNetwork(input_size=input_size, hidden_size=10, output_size=output_size, learning_rate=0.1)\n",
				"\n",
				"# Train\n",
				"print(\"Training model...\")\n",
				"epochs = 5000\n",
				"loss_history = []\n",
				"\n",
				"for i in range(epochs):\n",
				"    output = nn.train(X_train, y_train)\n",
				"    \n",
				"    # Calculate MSE Loss for monitoring (since we are using Sigmoid output)\n",
				"    loss = np.mean(np.square(y_train - output))\n",
				"    loss_history.append(loss)\n",
				"    \n",
				"    if i % 500 == 0:\n",
				"        print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
				"        \n",
				"print(f\"Final Loss: {loss:.4f}\")\n",
				"plot_loss(loss_history)\n",
				"\n",
				"# Evaluate\n",
				"print(\"\\nEvaluating model...\")\n",
				"y_pred_probs = nn.feedForward(X_test)\n",
				"y_pred = np.argmax(y_pred_probs, axis=1)\n",
				"y_true = np.argmax(y_test, axis=1)\n",
				"\n",
				"# Metrics\n",
				"accuracy = accuracy_score(y_true, y_pred)\n",
				"precision = precision_score(y_true, y_pred, average='weighted')\n",
				"recall = recall_score(y_true, y_pred, average='weighted')\n",
				"f1 = f1_score(y_true, y_pred, average='weighted')\n",
				"\n",
				"print(f\"Accuracy: {accuracy:.4f}\")\n",
				"print(f\"Precision: {precision:.4f}\")\n",
				"print(f\"Recall: {recall:.4f}\")\n",
				"print(f\"F1 Score: {f1:.4f}\")\n",
				"\n",
				"print(\"\\nClassification Report:\")\n",
				"print(classification_report(y_true, y_pred))\n",
				"\n",
				"# Confusion Matrix\n",
				"plot_confusion_matrix(y_true, y_pred, classes=[0, 1, 2, 3, 4])"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.5"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}